{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O0cwxevskAqy",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q7xwVYUYkNmJ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade transformers datasets accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GER0bfgnllYR",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fUKKQdBcvDCB",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Prerequisities\n",
    "\n",
    "1. Download conversations data and preprocess it to required format\n",
    "2. Create \"dataset\" dir and copy there all .txt files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RDNvbwpekw1a",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "from datasets import load_dataset\n",
    "\n",
    "# load text files into dataset\n",
    "dataset = load_dataset(\"text\", data_files={\"train\": glob.glob(\"dataset/*.txt\")})\n",
    "print(dataset)\n",
    "print(dataset[\"train\"][\"text\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JBWtsxlbtO7N",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bpUlL_DHlS9F",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# tokenize dataset\n",
    "# load tokenizer for model\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"flax-community/papuGaPT2\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7NVqIKqPme7D",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# add special tokens - <speaker1> and <speaker2> to tokenizer\n",
    "SPEAKER_TOKENS = {\"additional_special_tokens\": [\"<speaker1>\", \"<speaker2>\"]}\n",
    "tokenizer.add_special_tokens(SPEAKER_TOKENS)\n",
    "# resize model embeddings size - extend by 2 for speaker tokens\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "# set pad token as eos\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# transform text to list of subword token ids\n",
    "def tokenize_func(examples):\n",
    "    return tokenizer(examples[\"text\"])\n",
    "\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_func, batched=True, num_proc=4, remove_columns=[\"text\"]\n",
    ")\n",
    "tokenized_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(dataset[\"train\"][\"text\"][0])\n",
    "print(\n",
    "    f\"length of example (number of words): {len(dataset['train']['text'][0].split())}\"\n",
    ")\n",
    "print()\n",
    "print(tokenized_dataset[\"train\"][\"input_ids\"][0])\n",
    "print(f\"length of tokenized example: {len(tokenized_dataset['train']['input_ids'][0])}\")\n",
    "print()\n",
    "print(\"Decoded input ids by tokenizer\")\n",
    "print(tokenizer.decode(tokenized_dataset[\"train\"][\"input_ids\"][0]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Examples grouping**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# concatenate examples for largest blocks of text as possible for model, which is 512 tokens\n",
    "block_size = 512\n",
    "\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "\n",
    "language_model_dataset = tokenized_dataset.map(\n",
    "    group_texts, batched=True, batch_size=1000, num_proc=4\n",
    ")\n",
    "language_model_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f\"length of example: {len(language_model_dataset['train']['input_ids'][0])}\")\n",
    "print(f\"num of examples: {len(language_model_dataset['train']['input_ids'])}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# split to train and test dataset (which will be used as a validation dataset :)\n",
    "language_model_dataset_train_test = language_model_dataset[\"train\"].train_test_split(\n",
    "    test_size=0.1, seed=666\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "data_collator"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Training**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# training config\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    num_train_epochs=10,\n",
    "    learning_rate=1e-5,\n",
    "    weight_decay=0.01,\n",
    "    auto_find_batch_size=True,  # requires accelerate\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=language_model_dataset_train_test[\"train\"],\n",
    "    eval_dataset=language_model_dataset_train_test[\"test\"],\n",
    "    data_collator=data_collator,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# list callbacks\n",
    "trainer.callback_handler.callbacks"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# run training\n",
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# eval\n",
    "trainer.evaluate()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# play with custom inputs\n",
    "from transformers import pipeline, set_seed\n",
    "\n",
    "set_seed(93)\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=model.eval().cpu(), tokenizer=tokenizer)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "generator(\"<speaker1> hej<speaker2> czesc, co robisz w weekend?<speaker1>\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# advanced playing with custom inputs\n",
    "input_text = \"<speaker2> hej, p√≥jdziemy na piwko w czwartek?<speaker1>\"\n",
    "\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "sample_outputs = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True,\n",
    "    max_length=100,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    num_return_sequences=4,\n",
    ")\n",
    "\n",
    "print(f\"INPUT: {input_text}\")\n",
    "print(\"OUTPUT\")\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "    print(f\"{i}: {tokenizer.decode(sample_output)}\")\n",
    "    print()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C4FEsCuxuBFo",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "training_digital_avatar.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}